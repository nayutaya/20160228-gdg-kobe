2016年  2月 26日 金曜日 18:11:18 JST
time rake train
mkdir -p out/ec2-r3-xlarge
time python3 -u ../7_train.py tmp/dictionary.msgpack tmp/test.msgpack tmp/train.msgpack out/ec2-r3-xlarge/model out/ec2-r3-xlarge/data
dictfile  = tmp/dictionary.msgpack
testfile  = tmp/test.msgpack
trainfile = tmp/train.msgpack
modelfile = out/ec2-r3-xlarge/model
datadir   = out/ec2-r3-xlarge/data
loading...
test_records.len        = 2000
train_records.len       = 135390
train_rail_records.len  = 5321
train_other_records.len = 130069
model.optimizer         = <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>
model.num_of_categories = 2
model.num_of_terms      = 56686
training...
num_of_steps = 500
step#0
step#1
step#2
step#3
step#4
step#5
step#6
step#7
step#8
step#9
step#9 -> loss: 1264.8 / accuracy: 0.7495
step#10
step#11
step#12
step#13
step#14
step#15
step#16
step#17
step#18
step#19
step#19 -> loss: 1191.16 / accuracy: 0.763
step#20
step#21
step#22
step#23
step#24
step#25
step#26
step#27
step#28
step#29
step#29 -> loss: 1138.73 / accuracy: 0.7505
step#30
step#31
step#32
step#33
step#34
step#35
step#36
step#37
step#38
step#39
step#39 -> loss: 1098.17 / accuracy: 0.7555
step#40
step#41
step#42
step#43
step#44
step#45
step#46
step#47
step#48
step#49
step#49 -> loss: 1063.51 / accuracy: 0.7765
step#50
step#51
step#52
step#53
step#54
step#55
step#56
step#57
step#58
step#59
step#59 -> loss: 1035.88 / accuracy: 0.7995
step#60
step#61
step#62
step#63
step#64
step#65
step#66
step#67
step#68
step#69
step#69 -> loss: 1010.3 / accuracy: 0.7965
step#70
step#71
step#72
step#73
step#74
step#75
step#76
step#77
step#78
step#79
step#79 -> loss: 989.465 / accuracy: 0.793
step#80
step#81
step#82
step#83
step#84
step#85
step#86
step#87
step#88
step#89
step#89 -> loss: 967.346 / accuracy: 0.82
step#90
step#91
step#92
step#93
step#94
step#95
step#96
step#97
step#98
step#99
step#99 -> loss: 950.303 / accuracy: 0.8075
step#100
step#101
step#102
step#103
step#104
step#105
step#106
step#107
step#108
step#109
step#109 -> loss: 932.24 / accuracy: 0.8205
step#110
step#111
step#112
step#113
step#114
step#115
step#116
step#117
step#118
step#119
step#119 -> loss: 917.147 / accuracy: 0.816
step#120
step#121
step#122
step#123
step#124
step#125
step#126
step#127
step#128
step#129
step#129 -> loss: 900.496 / accuracy: 0.8305
step#130
step#131
step#132
step#133
step#134
step#135
step#136
step#137
step#138
step#139
step#139 -> loss: 886.485 / accuracy: 0.8315
step#140
step#141
step#142
step#143
step#144
step#145
step#146
step#147
step#148
step#149
step#149 -> loss: 873.832 / accuracy: 0.833
step#150
step#151
step#152
step#153
step#154
step#155
step#156
step#157
step#158
step#159
step#159 -> loss: 861.197 / accuracy: 0.838
step#160
step#161
step#162
step#163
step#164
step#165
step#166
step#167
step#168
step#169
step#169 -> loss: 851.73 / accuracy: 0.832
step#170
step#171
step#172
step#173
step#174
step#175
step#176
step#177
step#178
step#179
step#179 -> loss: 840.286 / accuracy: 0.8355
step#180
step#181
step#182
step#183
step#184
step#185
step#186
step#187
step#188
step#189
step#189 -> loss: 831.966 / accuracy: 0.832
step#190
step#191
step#192
step#193
step#194
step#195
step#196
step#197
step#198
step#199
step#199 -> loss: 819.893 / accuracy: 0.839
step#200
step#201
step#202
step#203
step#204
step#205
step#206
step#207
step#208
step#209
step#209 -> loss: 810.67 / accuracy: 0.8475
step#210
step#211
step#212
step#213
step#214
step#215
step#216
step#217
step#218
step#219
step#219 -> loss: 804.046 / accuracy: 0.838
step#220
step#221
step#222
step#223
step#224
step#225
step#226
step#227
step#228
step#229
step#229 -> loss: 798.538 / accuracy: 0.837
step#230
step#231
step#232
step#233
step#234
step#235
step#236
step#237
step#238
step#239
step#239 -> loss: 790.204 / accuracy: 0.8395
step#240
step#241
step#242
step#243
step#244
step#245
step#246
step#247
step#248
step#249
step#249 -> loss: 781.078 / accuracy: 0.8435
step#250
step#251
step#252
step#253
step#254
step#255
step#256
step#257
step#258
step#259
step#259 -> loss: 771.827 / accuracy: 0.8515
step#260
step#261
step#262
step#263
step#264
step#265
step#266
step#267
step#268
step#269
step#269 -> loss: 766.924 / accuracy: 0.8485
step#270
step#271
step#272
step#273
step#274
step#275
step#276
step#277
step#278
step#279
step#279 -> loss: 759.611 / accuracy: 0.8505
step#280
step#281
step#282
step#283
step#284
step#285
step#286
step#287
step#288
step#289
step#289 -> loss: 756.455 / accuracy: 0.8445
step#290
step#291
step#292
step#293
step#294
step#295
step#296
step#297
step#298
step#299
step#299 -> loss: 751.212 / accuracy: 0.8465
step#300
step#301
step#302
step#303
step#304
step#305
step#306
step#307
step#308
step#309
step#309 -> loss: 739.777 / accuracy: 0.8575
step#310
step#311
step#312
step#313
step#314
step#315
step#316
step#317
step#318
step#319
step#319 -> loss: 735.363 / accuracy: 0.8545
step#320
step#321
step#322
step#323
step#324
step#325
step#326
step#327
step#328
step#329
step#329 -> loss: 727.628 / accuracy: 0.8655
step#330
step#331
step#332
step#333
step#334
step#335
step#336
step#337
step#338
step#339
step#339 -> loss: 721.762 / accuracy: 0.8735
step#340
step#341
step#342
step#343
step#344
step#345
step#346
step#347
step#348
step#349
step#349 -> loss: 725.145 / accuracy: 0.8515
step#350
step#351
step#352
step#353
step#354
step#355
step#356
step#357
step#358
step#359
step#359 -> loss: 711.925 / accuracy: 0.873
step#360
step#361
step#362
step#363
step#364
step#365
step#366
step#367
step#368
step#369
step#369 -> loss: 711.219 / accuracy: 0.8585
step#370
step#371
step#372
step#373
step#374
step#375
step#376
step#377
step#378
step#379
step#379 -> loss: 703.392 / accuracy: 0.8675
step#380
step#381
step#382
step#383
step#384
step#385
step#386
step#387
step#388
step#389
step#389 -> loss: 699.31 / accuracy: 0.867
step#390
step#391
step#392
step#393
step#394
step#395
step#396
step#397
step#398
step#399
step#399 -> loss: 698.576 / accuracy: 0.862
step#400
step#401
step#402
step#403
step#404
step#405
step#406
step#407
step#408
step#409
step#409 -> loss: 697.068 / accuracy: 0.8575
step#410
step#411
step#412
step#413
step#414
step#415
step#416
step#417
step#418
step#419
step#419 -> loss: 684.578 / accuracy: 0.8735
step#420
step#421
step#422
step#423
step#424
step#425
step#426
step#427
step#428
step#429
step#429 -> loss: 686.87 / accuracy: 0.8625
step#430
step#431
step#432
step#433
step#434
step#435
step#436
step#437
step#438
step#439
step#439 -> loss: 678.981 / accuracy: 0.8685
step#440
step#441
step#442
step#443
step#444
step#445
step#446
step#447
step#448
step#449
step#449 -> loss: 676.424 / accuracy: 0.867
step#450
step#451
step#452
step#453
step#454
step#455
step#456
step#457
step#458
step#459
step#459 -> loss: 672.897 / accuracy: 0.8695
step#460
step#461
step#462
step#463
step#464
step#465
step#466
step#467
step#468
step#469
step#469 -> loss: 667.329 / accuracy: 0.871
step#470
step#471
step#472
step#473
step#474
step#475
step#476
step#477
step#478
step#479
step#479 -> loss: 665.812 / accuracy: 0.87
step#480
step#481
step#482
step#483
step#484
step#485
step#486
step#487
step#488
step#489
step#489 -> loss: 659.888 / accuracy: 0.872
step#490
step#491
step#492
step#493
step#494
step#495
step#496
step#497
step#498
step#499
step#499 -> loss: 657.205 / accuracy: 0.8715
586.18user 62.55system 10:27.28elapsed 103%CPU (0avgtext+0avgdata 2844628maxresident)k
0inputs+1016outputs (0major+27645491minor)pagefaults 0swaps
586.29user 62.56system 10:27.41elapsed 103%CPU (0avgtext+0avgdata 2844628maxresident)k
0inputs+1016outputs (0major+27649111minor)pagefaults 0swaps
2016年  2月 26日 金曜日 18:21:45 JST
