2016年  2月 26日 金曜日 17:53:39 JST
time rake train
mkdir -p out/ec2-r3-xlarge
time python3 -u ../7_train.py tmp/dictionary.msgpack tmp/test.msgpack tmp/train.msgpack out/ec2-r3-xlarge/model out/ec2-r3-xlarge/data
dictfile  = tmp/dictionary.msgpack
testfile  = tmp/test.msgpack
trainfile = tmp/train.msgpack
modelfile = out/ec2-r3-xlarge/model
datadir   = out/ec2-r3-xlarge/data
loading...
test_records.len        = 2000
train_records.len       = 135390
train_rail_records.len  = 5321
train_other_records.len = 130069
model.optimizer         = <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>
model.num_of_categories = 2
model.num_of_terms      = 2553
training...
num_of_steps = 500
step#0
step#1
step#2
step#3
step#4
step#5
step#6
step#7
step#8
step#9
step#9 -> loss: 653.305 / accuracy: 0.9195
step#10
step#11
step#12
step#13
step#14
step#15
step#16
step#17
step#18
step#19
step#19 -> loss: 520.728 / accuracy: 0.916
step#20
step#21
step#22
step#23
step#24
step#25
step#26
step#27
step#28
step#29
step#29 -> loss: 448.021 / accuracy: 0.931
step#30
step#31
step#32
step#33
step#34
step#35
step#36
step#37
step#38
step#39
step#39 -> loss: 402.785 / accuracy: 0.941
step#40
step#41
step#42
step#43
step#44
step#45
step#46
step#47
step#48
step#49
step#49 -> loss: 371.537 / accuracy: 0.9475
step#50
step#51
step#52
step#53
step#54
step#55
step#56
step#57
step#58
step#59
step#59 -> loss: 347.382 / accuracy: 0.951
step#60
step#61
step#62
step#63
step#64
step#65
step#66
step#67
step#68
step#69
step#69 -> loss: 336.329 / accuracy: 0.95
step#70
step#71
step#72
step#73
step#74
step#75
step#76
step#77
step#78
step#79
step#79 -> loss: 319.296 / accuracy: 0.9555
step#80
step#81
step#82
step#83
step#84
step#85
step#86
step#87
step#88
step#89
step#89 -> loss: 307.815 / accuracy: 0.957
step#90
step#91
step#92
step#93
step#94
step#95
step#96
step#97
step#98
step#99
step#99 -> loss: 297.968 / accuracy: 0.9545
step#100
step#101
step#102
step#103
step#104
step#105
step#106
step#107
step#108
step#109
step#109 -> loss: 289.451 / accuracy: 0.957
step#110
step#111
step#112
step#113
step#114
step#115
step#116
step#117
step#118
step#119
step#119 -> loss: 282.758 / accuracy: 0.9575
step#120
step#121
step#122
step#123
step#124
step#125
step#126
step#127
step#128
step#129
step#129 -> loss: 275.088 / accuracy: 0.959
step#130
step#131
step#132
step#133
step#134
step#135
step#136
step#137
step#138
step#139
step#139 -> loss: 267.755 / accuracy: 0.96
step#140
step#141
step#142
step#143
step#144
step#145
step#146
step#147
step#148
step#149
step#149 -> loss: 260.477 / accuracy: 0.9605
step#150
step#151
step#152
step#153
step#154
step#155
step#156
step#157
step#158
step#159
step#159 -> loss: 256.757 / accuracy: 0.96
step#160
step#161
step#162
step#163
step#164
step#165
step#166
step#167
step#168
step#169
step#169 -> loss: 254.663 / accuracy: 0.958
step#170
step#171
step#172
step#173
step#174
step#175
step#176
step#177
step#178
step#179
step#179 -> loss: 248.547 / accuracy: 0.958
step#180
step#181
step#182
step#183
step#184
step#185
step#186
step#187
step#188
step#189
step#189 -> loss: 244.766 / accuracy: 0.9575
step#190
step#191
step#192
step#193
step#194
step#195
step#196
step#197
step#198
step#199
step#199 -> loss: 238.68 / accuracy: 0.9605
step#200
step#201
step#202
step#203
step#204
step#205
step#206
step#207
step#208
step#209
step#209 -> loss: 234.522 / accuracy: 0.961
step#210
step#211
step#212
step#213
step#214
step#215
step#216
step#217
step#218
step#219
step#219 -> loss: 234.175 / accuracy: 0.9595
step#220
step#221
step#222
step#223
step#224
step#225
step#226
step#227
step#228
step#229
step#229 -> loss: 231.243 / accuracy: 0.959
step#230
step#231
step#232
step#233
step#234
step#235
step#236
step#237
step#238
step#239
step#239 -> loss: 234.976 / accuracy: 0.96
step#240
step#241
step#242
step#243
step#244
step#245
step#246
step#247
step#248
step#249
step#249 -> loss: 229.432 / accuracy: 0.9605
step#250
step#251
step#252
step#253
step#254
step#255
step#256
step#257
step#258
step#259
step#259 -> loss: 231.968 / accuracy: 0.959
step#260
step#261
step#262
step#263
step#264
step#265
step#266
step#267
step#268
step#269
step#269 -> loss: 222.422 / accuracy: 0.962
step#270
step#271
step#272
step#273
step#274
step#275
step#276
step#277
step#278
step#279
step#279 -> loss: 222.666 / accuracy: 0.9615
step#280
step#281
step#282
step#283
step#284
step#285
step#286
step#287
step#288
step#289
step#289 -> loss: 220.14 / accuracy: 0.961
step#290
step#291
step#292
step#293
step#294
step#295
step#296
step#297
step#298
step#299
step#299 -> loss: 220.017 / accuracy: 0.9605
step#300
step#301
step#302
step#303
step#304
step#305
step#306
step#307
step#308
step#309
step#309 -> loss: 224.904 / accuracy: 0.958
step#310
step#311
step#312
step#313
step#314
step#315
step#316
step#317
step#318
step#319
step#319 -> loss: 214.574 / accuracy: 0.963
step#320
step#321
step#322
step#323
step#324
step#325
step#326
step#327
step#328
step#329
step#329 -> loss: 213.792 / accuracy: 0.963
step#330
step#331
step#332
step#333
step#334
step#335
step#336
step#337
step#338
step#339
step#339 -> loss: 211.51 / accuracy: 0.9635
step#340
step#341
step#342
step#343
step#344
step#345
step#346
step#347
step#348
step#349
step#349 -> loss: 213.948 / accuracy: 0.9605
step#350
step#351
step#352
step#353
step#354
step#355
step#356
step#357
step#358
step#359
step#359 -> loss: 207.352 / accuracy: 0.967
step#360
step#361
step#362
step#363
step#364
step#365
step#366
step#367
step#368
step#369
step#369 -> loss: 208.465 / accuracy: 0.9635
step#370
step#371
step#372
step#373
step#374
step#375
step#376
step#377
step#378
step#379
step#379 -> loss: 209.53 / accuracy: 0.9645
step#380
step#381
step#382
step#383
step#384
step#385
step#386
step#387
step#388
step#389
step#389 -> loss: 205.101 / accuracy: 0.964
step#390
step#391
step#392
step#393
step#394
step#395
step#396
step#397
step#398
step#399
step#399 -> loss: 211.838 / accuracy: 0.962
step#400
step#401
step#402
step#403
step#404
step#405
step#406
step#407
step#408
step#409
step#409 -> loss: 206.78 / accuracy: 0.9625
step#410
step#411
step#412
step#413
step#414
step#415
step#416
step#417
step#418
step#419
step#419 -> loss: 206.199 / accuracy: 0.963
step#420
step#421
step#422
step#423
step#424
step#425
step#426
step#427
step#428
step#429
step#429 -> loss: 205.548 / accuracy: 0.9645
step#430
step#431
step#432
step#433
step#434
step#435
step#436
step#437
step#438
step#439
step#439 -> loss: 204.751 / accuracy: 0.9645
step#440
step#441
step#442
step#443
step#444
step#445
step#446
step#447
step#448
step#449
step#449 -> loss: 206.874 / accuracy: 0.9625
step#450
step#451
step#452
step#453
step#454
step#455
step#456
step#457
step#458
step#459
step#459 -> loss: 198.412 / accuracy: 0.9685
step#460
step#461
step#462
step#463
step#464
step#465
step#466
step#467
step#468
step#469
step#469 -> loss: 202.538 / accuracy: 0.967
step#470
step#471
step#472
step#473
step#474
step#475
step#476
step#477
step#478
step#479
step#479 -> loss: 200.587 / accuracy: 0.9685
step#480
step#481
step#482
step#483
step#484
step#485
step#486
step#487
step#488
step#489
step#489 -> loss: 196.81 / accuracy: 0.968
step#490
step#491
step#492
step#493
step#494
step#495
step#496
step#497
step#498
step#499
step#499 -> loss: 195.647 / accuracy: 0.9685
121.26user 1.19system 1:59.37elapsed 102%CPU (0avgtext+0avgdata 561408maxresident)k
0inputs+152outputs (0major+386706minor)pagefaults 0swaps
121.31user 1.21system 1:59.45elapsed 102%CPU (0avgtext+0avgdata 561408maxresident)k
0inputs+152outputs (0major+390334minor)pagefaults 0swaps
2016年  2月 26日 金曜日 17:55:39 JST
