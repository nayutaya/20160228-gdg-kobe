2016年  2月 26日 金曜日 18:04:17 JST
time rake train
mkdir -p out/ec2-r3-xlarge
time python3 -u ../7_train.py tmp/dictionary.msgpack tmp/test.msgpack tmp/train.msgpack out/ec2-r3-xlarge/model out/ec2-r3-xlarge/data
dictfile  = tmp/dictionary.msgpack
testfile  = tmp/test.msgpack
trainfile = tmp/train.msgpack
modelfile = out/ec2-r3-xlarge/model
datadir   = out/ec2-r3-xlarge/data
loading...
test_records.len        = 2000
train_records.len       = 135390
train_rail_records.len  = 5321
train_other_records.len = 130069
model.optimizer           = <class 'tensorflow.python.training.adam.AdamOptimizer'>
model.num_of_categories   = 2
model.num_of_terms        = 60728
model.num_of_hidden_nodes = 100
training...
num_of_steps = 500
step#0
step#1
step#2
step#3
step#4
step#5
step#6
step#7
step#8
step#9
step#9 -> loss: 501.497 / accuracy: 0.912
step#10
step#11
step#12
step#13
step#14
step#15
step#16
step#17
step#18
step#19
step#19 -> loss: 407.087 / accuracy: 0.932
step#20
step#21
step#22
step#23
step#24
step#25
step#26
step#27
step#28
step#29
step#29 -> loss: 365.608 / accuracy: 0.948
step#30
step#31
step#32
step#33
step#34
step#35
step#36
step#37
step#38
step#39
step#39 -> loss: 323.731 / accuracy: 0.9485
step#40
step#41
step#42
step#43
step#44
step#45
step#46
step#47
step#48
step#49
step#49 -> loss: 319.22 / accuracy: 0.9515
step#50
step#51
step#52
step#53
step#54
step#55
step#56
step#57
step#58
step#59
step#59 -> loss: 270.114 / accuracy: 0.956
step#60
step#61
step#62
step#63
step#64
step#65
step#66
step#67
step#68
step#69
step#69 -> loss: 260.633 / accuracy: 0.9585
step#70
step#71
step#72
step#73
step#74
step#75
step#76
step#77
step#78
step#79
step#79 -> loss: 274.276 / accuracy: 0.955
step#80
step#81
step#82
step#83
step#84
step#85
step#86
step#87
step#88
step#89
step#89 -> loss: 258.371 / accuracy: 0.959
step#90
step#91
step#92
step#93
step#94
step#95
step#96
step#97
step#98
step#99
step#99 -> loss: 241.8 / accuracy: 0.959
step#100
step#101
step#102
step#103
step#104
step#105
step#106
step#107
step#108
step#109
step#109 -> loss: 264.383 / accuracy: 0.955
step#110
step#111
step#112
step#113
step#114
step#115
step#116
step#117
step#118
step#119
step#119 -> loss: 271.585 / accuracy: 0.952
step#120
step#121
step#122
step#123
step#124
step#125
step#126
step#127
step#128
step#129
step#129 -> loss: 349.192 / accuracy: 0.941
step#130
step#131
step#132
step#133
step#134
step#135
step#136
step#137
step#138
step#139
step#139 -> loss: 285.298 / accuracy: 0.9515
step#140
step#141
step#142
step#143
step#144
step#145
step#146
step#147
step#148
step#149
step#149 -> loss: 261.949 / accuracy: 0.958
step#150
step#151
step#152
step#153
step#154
step#155
step#156
step#157
step#158
step#159
step#159 -> loss: 247.209 / accuracy: 0.9565
step#160
step#161
step#162
step#163
step#164
step#165
step#166
step#167
step#168
step#169
step#169 -> loss: 284.954 / accuracy: 0.949
step#170
step#171
step#172
step#173
step#174
step#175
step#176
step#177
step#178
step#179
step#179 -> loss: 249.874 / accuracy: 0.9575
step#180
step#181
step#182
step#183
step#184
step#185
step#186
step#187
step#188
step#189
step#189 -> loss: 248.584 / accuracy: 0.9565
step#190
step#191
step#192
step#193
step#194
step#195
step#196
step#197
step#198
step#199
step#199 -> loss: 299.988 / accuracy: 0.951
step#200
step#201
step#202
step#203
step#204
step#205
step#206
step#207
step#208
step#209
step#209 -> loss: 258.368 / accuracy: 0.96
step#210
step#211
step#212
step#213
step#214
step#215
step#216
step#217
step#218
step#219
step#219 -> loss: 268.691 / accuracy: 0.958
step#220
step#221
step#222
step#223
step#224
step#225
step#226
step#227
step#228
step#229
step#229 -> loss: 266.716 / accuracy: 0.9545
step#230
step#231
step#232
step#233
step#234
step#235
step#236
step#237
step#238
step#239
step#239 -> loss: 269.055 / accuracy: 0.956
step#240
step#241
step#242
step#243
step#244
step#245
step#246
step#247
step#248
step#249
step#249 -> loss: 256.624 / accuracy: 0.957
step#250
step#251
step#252
step#253
step#254
step#255
step#256
step#257
step#258
step#259
step#259 -> loss: 272.887 / accuracy: 0.9575
step#260
step#261
step#262
step#263
step#264
step#265
step#266
step#267
step#268
step#269
step#269 -> loss: 272.233 / accuracy: 0.956
step#270
step#271
step#272
step#273
step#274
step#275
step#276
step#277
step#278
step#279
step#279 -> loss: 285.14 / accuracy: 0.9555
step#280
step#281
step#282
step#283
step#284
step#285
step#286
step#287
step#288
step#289
step#289 -> loss: 253.338 / accuracy: 0.959
step#290
step#291
step#292
step#293
step#294
step#295
step#296
step#297
step#298
step#299
step#299 -> loss: 245.351 / accuracy: 0.958
step#300
step#301
step#302
step#303
step#304
step#305
step#306
step#307
step#308
step#309
step#309 -> loss: 231.394 / accuracy: 0.961
step#310
step#311
step#312
step#313
step#314
step#315
step#316
step#317
step#318
step#319
step#319 -> loss: 262.166 / accuracy: 0.9565
step#320
step#321
step#322
step#323
step#324
step#325
step#326
step#327
step#328
step#329
step#329 -> loss: 279.624 / accuracy: 0.954
step#330
step#331
step#332
step#333
step#334
step#335
step#336
step#337
step#338
step#339
step#339 -> loss: 237.162 / accuracy: 0.9605
step#340
step#341
step#342
step#343
step#344
step#345
step#346
step#347
step#348
step#349
step#349 -> loss: 322.144 / accuracy: 0.953
step#350
step#351
step#352
step#353
step#354
step#355
step#356
step#357
step#358
step#359
step#359 -> loss: 296.688 / accuracy: 0.9595
step#360
step#361
step#362
step#363
step#364
step#365
step#366
step#367
step#368
step#369
step#369 -> loss: 262.925 / accuracy: 0.956
step#370
step#371
step#372
step#373
step#374
step#375
step#376
step#377
step#378
step#379
step#379 -> loss: 343.461 / accuracy: 0.953
step#380
step#381
step#382
step#383
step#384
step#385
step#386
step#387
step#388
step#389
step#389 -> loss: 354.39 / accuracy: 0.951
step#390
step#391
step#392
step#393
step#394
step#395
step#396
step#397
step#398
step#399
step#399 -> loss: 331.735 / accuracy: 0.955
step#400
step#401
step#402
step#403
step#404
step#405
step#406
step#407
step#408
step#409
step#409 -> loss: 308.79 / accuracy: 0.9545
step#410
step#411
step#412
step#413
step#414
step#415
step#416
step#417
step#418
step#419
step#419 -> loss: 323.858 / accuracy: 0.9545
step#420
step#421
step#422
step#423
step#424
step#425
step#426
step#427
step#428
step#429
step#429 -> loss: 343.312 / accuracy: 0.955
step#430
step#431
step#432
step#433
step#434
step#435
step#436
step#437
step#438
step#439
step#439 -> loss: 321.718 / accuracy: 0.954
step#440
step#441
step#442
step#443
step#444
step#445
step#446
step#447
step#448
step#449
step#449 -> loss: 304.91 / accuracy: 0.955
step#450
step#451
step#452
step#453
step#454
step#455
step#456
step#457
step#458
step#459
step#459 -> loss: 245.879 / accuracy: 0.961
step#460
step#461
step#462
step#463
step#464
step#465
step#466
step#467
step#468
step#469
step#469 -> loss: 241.222 / accuracy: 0.9615
step#470
step#471
step#472
step#473
step#474
step#475
step#476
step#477
step#478
step#479
step#479 -> loss: 284.723 / accuracy: 0.956
step#480
step#481
step#482
step#483
step#484
step#485
step#486
step#487
step#488
step#489
step#489 -> loss: 234.91 / accuracy: 0.9615
step#490
step#491
step#492
step#493
step#494
step#495
step#496
step#497
step#498
step#499
step#499 -> loss: 229.177 / accuracy: 0.9595
1227.15user 88.71system 15:53.72elapsed 137%CPU (0avgtext+0avgdata 3113892maxresident)k
8inputs+142592outputs (0major+33938306minor)pagefaults 0swaps
1227.24user 88.72system 15:53.82elapsed 137%CPU (0avgtext+0avgdata 3113892maxresident)k
8inputs+142592outputs (0major+33941932minor)pagefaults 0swaps
2016年  2月 26日 金曜日 18:20:11 JST
