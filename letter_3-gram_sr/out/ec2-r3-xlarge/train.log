2016年  2月 26日 金曜日 18:12:13 JST
time rake train
mkdir -p out/ec2-r3-xlarge
time python3 -u ../7_train.py tmp/dictionary.msgpack tmp/test.msgpack tmp/train.msgpack out/ec2-r3-xlarge/model out/ec2-r3-xlarge/data
dictfile  = tmp/dictionary.msgpack
testfile  = tmp/test.msgpack
trainfile = tmp/train.msgpack
modelfile = out/ec2-r3-xlarge/model
datadir   = out/ec2-r3-xlarge/data
loading...
test_records.len        = 2000
train_records.len       = 135390
train_rail_records.len  = 5321
train_other_records.len = 130069
model.optimizer         = <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>
model.num_of_categories = 2
model.num_of_terms      = 95537
training...
num_of_steps = 500
step#0
step#1
step#2
step#3
step#4
step#5
step#6
step#7
step#8
step#9
step#9 -> loss: 1156.92 / accuracy: 0.7765
step#10
step#11
step#12
step#13
step#14
step#15
step#16
step#17
step#18
step#19
step#19 -> loss: 1056.64 / accuracy: 0.7885
step#20
step#21
step#22
step#23
step#24
step#25
step#26
step#27
step#28
step#29
step#29 -> loss: 989.271 / accuracy: 0.79
step#30
step#31
step#32
step#33
step#34
step#35
step#36
step#37
step#38
step#39
step#39 -> loss: 938.742 / accuracy: 0.7985
step#40
step#41
step#42
step#43
step#44
step#45
step#46
step#47
step#48
step#49
step#49 -> loss: 897.295 / accuracy: 0.819
step#50
step#51
step#52
step#53
step#54
step#55
step#56
step#57
step#58
step#59
step#59 -> loss: 865.33 / accuracy: 0.836
step#60
step#61
step#62
step#63
step#64
step#65
step#66
step#67
step#68
step#69
step#69 -> loss: 836.464 / accuracy: 0.8405
step#70
step#71
step#72
step#73
step#74
step#75
step#76
step#77
step#78
step#79
step#79 -> loss: 813.246 / accuracy: 0.8335
step#80
step#81
step#82
step#83
step#84
step#85
step#86
step#87
step#88
step#89
step#89 -> loss: 789.525 / accuracy: 0.8515
step#90
step#91
step#92
step#93
step#94
step#95
step#96
step#97
step#98
step#99
step#99 -> loss: 771.655 / accuracy: 0.846
step#100
step#101
step#102
step#103
step#104
step#105
step#106
step#107
step#108
step#109
step#109 -> loss: 752.854 / accuracy: 0.854
step#110
step#111
step#112
step#113
step#114
step#115
step#116
step#117
step#118
step#119
step#119 -> loss: 738.739 / accuracy: 0.8525
step#120
step#121
step#122
step#123
step#124
step#125
step#126
step#127
step#128
step#129
step#129 -> loss: 722.468 / accuracy: 0.856
step#130
step#131
step#132
step#133
step#134
step#135
step#136
step#137
step#138
step#139
step#139 -> loss: 706.998 / accuracy: 0.861
step#140
step#141
step#142
step#143
step#144
step#145
step#146
step#147
step#148
step#149
step#149 -> loss: 694.978 / accuracy: 0.86
step#150
step#151
step#152
step#153
step#154
step#155
step#156
step#157
step#158
step#159
step#159 -> loss: 682.04 / accuracy: 0.8695
step#160
step#161
step#162
step#163
step#164
step#165
step#166
step#167
step#168
step#169
step#169 -> loss: 673.825 / accuracy: 0.8635
step#170
step#171
step#172
step#173
step#174
step#175
step#176
step#177
step#178
step#179
step#179 -> loss: 663.343 / accuracy: 0.868
step#180
step#181
step#182
step#183
step#184
step#185
step#186
step#187
step#188
step#189
step#189 -> loss: 654.673 / accuracy: 0.8685
step#190
step#191
step#192
step#193
step#194
step#195
step#196
step#197
step#198
step#199
step#199 -> loss: 643.703 / accuracy: 0.8785
step#200
step#201
step#202
step#203
step#204
step#205
step#206
step#207
step#208
step#209
step#209 -> loss: 635.6 / accuracy: 0.879
step#210
step#211
step#212
step#213
step#214
step#215
step#216
step#217
step#218
step#219
step#219 -> loss: 629.556 / accuracy: 0.872
step#220
step#221
step#222
step#223
step#224
step#225
step#226
step#227
step#228
step#229
step#229 -> loss: 623.085 / accuracy: 0.875
step#230
step#231
step#232
step#233
step#234
step#235
step#236
step#237
step#238
step#239
step#239 -> loss: 617.159 / accuracy: 0.875
step#240
step#241
step#242
step#243
step#244
step#245
step#246
step#247
step#248
step#249
step#249 -> loss: 611.67 / accuracy: 0.8745
step#250
step#251
step#252
step#253
step#254
step#255
step#256
step#257
step#258
step#259
step#259 -> loss: 600.518 / accuracy: 0.8895
step#260
step#261
step#262
step#263
step#264
step#265
step#266
step#267
step#268
step#269
step#269 -> loss: 595.228 / accuracy: 0.8845
step#270
step#271
step#272
step#273
step#274
step#275
step#276
step#277
step#278
step#279
step#279 -> loss: 590.113 / accuracy: 0.885
step#280
step#281
step#282
step#283
step#284
step#285
step#286
step#287
step#288
step#289
step#289 -> loss: 585.248 / accuracy: 0.8845
step#290
step#291
step#292
step#293
step#294
step#295
step#296
step#297
step#298
step#299
step#299 -> loss: 581.689 / accuracy: 0.8825
step#300
step#301
step#302
step#303
step#304
step#305
step#306
step#307
step#308
step#309
step#309 -> loss: 573.438 / accuracy: 0.8915
step#310
step#311
step#312
step#313
step#314
step#315
step#316
step#317
step#318
step#319
step#319 -> loss: 569.58 / accuracy: 0.891
step#320
step#321
step#322
step#323
step#324
step#325
step#326
step#327
step#328
step#329
step#329 -> loss: 564.071 / accuracy: 0.893
step#330
step#331
step#332
step#333
step#334
step#335
step#336
step#337
step#338
step#339
step#339 -> loss: 557.862 / accuracy: 0.898
step#340
step#341
step#342
step#343
step#344
step#345
step#346
step#347
step#348
step#349
step#349 -> loss: 560.655 / accuracy: 0.8855
step#350
step#351
step#352
step#353
step#354
step#355
step#356
step#357
step#358
step#359
step#359 -> loss: 548.054 / accuracy: 0.901
step#360
step#361
step#362
step#363
step#364
step#365
step#366
step#367
step#368
step#369
step#369 -> loss: 548.344 / accuracy: 0.8925
step#370
step#371
step#372
step#373
step#374
step#375
step#376
step#377
step#378
step#379
step#379 -> loss: 540.645 / accuracy: 0.9005
step#380
step#381
step#382
step#383
step#384
step#385
step#386
step#387
step#388
step#389
step#389 -> loss: 538.65 / accuracy: 0.8975
step#390
step#391
step#392
step#393
step#394
step#395
step#396
step#397
step#398
step#399
step#399 -> loss: 537.998 / accuracy: 0.8935
step#400
step#401
step#402
step#403
step#404
step#405
step#406
step#407
step#408
step#409
step#409 -> loss: 537.636 / accuracy: 0.891
step#410
step#411
step#412
step#413
step#414
step#415
step#416
step#417
step#418
step#419
step#419 -> loss: 527.057 / accuracy: 0.9
step#420
step#421
step#422
step#423
step#424
step#425
step#426
step#427
step#428
step#429
step#429 -> loss: 527.792 / accuracy: 0.897
step#430
step#431
step#432
step#433
step#434
step#435
step#436
step#437
step#438
step#439
step#439 -> loss: 520.36 / accuracy: 0.9
step#440
step#441
step#442
step#443
step#444
step#445
step#446
step#447
step#448
step#449
step#449 -> loss: 524.164 / accuracy: 0.8945
step#450
step#451
step#452
step#453
step#454
step#455
step#456
step#457
step#458
step#459
step#459 -> loss: 514.617 / accuracy: 0.902
step#460
step#461
step#462
step#463
step#464
step#465
step#466
step#467
step#468
step#469
step#469 -> loss: 509.74 / accuracy: 0.9045
step#470
step#471
step#472
step#473
step#474
step#475
step#476
step#477
step#478
step#479
step#479 -> loss: 510.519 / accuracy: 0.9015
step#480
step#481
step#482
step#483
step#484
step#485
step#486
step#487
step#488
step#489
step#489 -> loss: 507.673 / accuracy: 0.9015
step#490
step#491
step#492
step#493
step#494
step#495
step#496
step#497
step#498
step#499
step#499 -> loss: 502.54 / accuracy: 0.903
805.85user 97.91system 13:59.92elapsed 107%CPU (0avgtext+0avgdata 4312224maxresident)k
0inputs+1640outputs (0major+46401681minor)pagefaults 0swaps
805.96user 97.92system 14:00.04elapsed 107%CPU (0avgtext+0avgdata 4312224maxresident)k
0inputs+1640outputs (0major+46405308minor)pagefaults 0swaps
2016年  2月 26日 金曜日 18:26:13 JST
