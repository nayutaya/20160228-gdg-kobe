2016年  2月 26日 金曜日 18:05:17 JST
time rake train
mkdir -p out/ec2-r3-xlarge
time python3 -u ../7_train.py tmp/dictionary.msgpack tmp/test.msgpack tmp/train.msgpack out/ec2-r3-xlarge/model out/ec2-r3-xlarge/data
dictfile  = tmp/dictionary.msgpack
testfile  = tmp/test.msgpack
trainfile = tmp/train.msgpack
modelfile = out/ec2-r3-xlarge/model
datadir   = out/ec2-r3-xlarge/data
loading...
test_records.len        = 2000
train_records.len       = 135390
train_rail_records.len  = 5321
train_other_records.len = 130069
model.optimizer         = <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>
model.num_of_categories = 2
model.num_of_terms      = 16846
training...
num_of_steps = 500
step#0
step#1
step#2
step#3
step#4
step#5
step#6
step#7
step#8
step#9
step#9 -> loss: 958.548 / accuracy: 0.869
step#10
step#11
step#12
step#13
step#14
step#15
step#16
step#17
step#18
step#19
step#19 -> loss: 798.422 / accuracy: 0.8745
step#20
step#21
step#22
step#23
step#24
step#25
step#26
step#27
step#28
step#29
step#29 -> loss: 710.49 / accuracy: 0.885
step#30
step#31
step#32
step#33
step#34
step#35
step#36
step#37
step#38
step#39
step#39 -> loss: 649.428 / accuracy: 0.8975
step#40
step#41
step#42
step#43
step#44
step#45
step#46
step#47
step#48
step#49
step#49 -> loss: 602.598 / accuracy: 0.9165
step#50
step#51
step#52
step#53
step#54
step#55
step#56
step#57
step#58
step#59
step#59 -> loss: 566.065 / accuracy: 0.9245
step#60
step#61
step#62
step#63
step#64
step#65
step#66
step#67
step#68
step#69
step#69 -> loss: 535.571 / accuracy: 0.923
step#70
step#71
step#72
step#73
step#74
step#75
step#76
step#77
step#78
step#79
step#79 -> loss: 510.466 / accuracy: 0.9235
step#80
step#81
step#82
step#83
step#84
step#85
step#86
step#87
step#88
step#89
step#89 -> loss: 488.124 / accuracy: 0.9395
step#90
step#91
step#92
step#93
step#94
step#95
step#96
step#97
step#98
step#99
step#99 -> loss: 469.998 / accuracy: 0.9325
step#100
step#101
step#102
step#103
step#104
step#105
step#106
step#107
step#108
step#109
step#109 -> loss: 451.754 / accuracy: 0.9345
step#110
step#111
step#112
step#113
step#114
step#115
step#116
step#117
step#118
step#119
step#119 -> loss: 438.269 / accuracy: 0.9345
step#120
step#121
step#122
step#123
step#124
step#125
step#126
step#127
step#128
step#129
step#129 -> loss: 426.719 / accuracy: 0.9345
step#130
step#131
step#132
step#133
step#134
step#135
step#136
step#137
step#138
step#139
step#139 -> loss: 412.007 / accuracy: 0.9435
step#140
step#141
step#142
step#143
step#144
step#145
step#146
step#147
step#148
step#149
step#149 -> loss: 402.432 / accuracy: 0.9445
step#150
step#151
step#152
step#153
step#154
step#155
step#156
step#157
step#158
step#159
step#159 -> loss: 392.566 / accuracy: 0.943
step#160
step#161
step#162
step#163
step#164
step#165
step#166
step#167
step#168
step#169
step#169 -> loss: 384.705 / accuracy: 0.939
step#170
step#171
step#172
step#173
step#174
step#175
step#176
step#177
step#178
step#179
step#179 -> loss: 375.762 / accuracy: 0.942
step#180
step#181
step#182
step#183
step#184
step#185
step#186
step#187
step#188
step#189
step#189 -> loss: 368.976 / accuracy: 0.9435
step#190
step#191
step#192
step#193
step#194
step#195
step#196
step#197
step#198
step#199
step#199 -> loss: 359.849 / accuracy: 0.9475
step#200
step#201
step#202
step#203
step#204
step#205
step#206
step#207
step#208
step#209
step#209 -> loss: 354.404 / accuracy: 0.947
step#210
step#211
step#212
step#213
step#214
step#215
step#216
step#217
step#218
step#219
step#219 -> loss: 349.718 / accuracy: 0.9495
step#220
step#221
step#222
step#223
step#224
step#225
step#226
step#227
step#228
step#229
step#229 -> loss: 342.77 / accuracy: 0.946
step#230
step#231
step#232
step#233
step#234
step#235
step#236
step#237
step#238
step#239
step#239 -> loss: 339.106 / accuracy: 0.9465
step#240
step#241
step#242
step#243
step#244
step#245
step#246
step#247
step#248
step#249
step#249 -> loss: 333.473 / accuracy: 0.947
step#250
step#251
step#252
step#253
step#254
step#255
step#256
step#257
step#258
step#259
step#259 -> loss: 332.111 / accuracy: 0.944
step#260
step#261
step#262
step#263
step#264
step#265
step#266
step#267
step#268
step#269
step#269 -> loss: 323.387 / accuracy: 0.9495
step#270
step#271
step#272
step#273
step#274
step#275
step#276
step#277
step#278
step#279
step#279 -> loss: 318.508 / accuracy: 0.95
step#280
step#281
step#282
step#283
step#284
step#285
step#286
step#287
step#288
step#289
step#289 -> loss: 318.525 / accuracy: 0.9465
step#290
step#291
step#292
step#293
step#294
step#295
step#296
step#297
step#298
step#299
step#299 -> loss: 315.359 / accuracy: 0.948
step#300
step#301
step#302
step#303
step#304
step#305
step#306
step#307
step#308
step#309
step#309 -> loss: 306.633 / accuracy: 0.949
step#310
step#311
step#312
step#313
step#314
step#315
step#316
step#317
step#318
step#319
step#319 -> loss: 302.375 / accuracy: 0.954
step#320
step#321
step#322
step#323
step#324
step#325
step#326
step#327
step#328
step#329
step#329 -> loss: 298.606 / accuracy: 0.957
step#330
step#331
step#332
step#333
step#334
step#335
step#336
step#337
step#338
step#339
step#339 -> loss: 294.85 / accuracy: 0.9575
step#340
step#341
step#342
step#343
step#344
step#345
step#346
step#347
step#348
step#349
step#349 -> loss: 300.522 / accuracy: 0.947
step#350
step#351
step#352
step#353
step#354
step#355
step#356
step#357
step#358
step#359
step#359 -> loss: 289.116 / accuracy: 0.9605
step#360
step#361
step#362
step#363
step#364
step#365
step#366
step#367
step#368
step#369
step#369 -> loss: 290.005 / accuracy: 0.9495
step#370
step#371
step#372
step#373
step#374
step#375
step#376
step#377
step#378
step#379
step#379 -> loss: 285.768 / accuracy: 0.9545
step#380
step#381
step#382
step#383
step#384
step#385
step#386
step#387
step#388
step#389
step#389 -> loss: 282.036 / accuracy: 0.956
step#390
step#391
step#392
step#393
step#394
step#395
step#396
step#397
step#398
step#399
step#399 -> loss: 283.144 / accuracy: 0.9535
step#400
step#401
step#402
step#403
step#404
step#405
step#406
step#407
step#408
step#409
step#409 -> loss: 277.57 / accuracy: 0.9555
step#410
step#411
step#412
step#413
step#414
step#415
step#416
step#417
step#418
step#419
step#419 -> loss: 272.313 / accuracy: 0.9605
step#420
step#421
step#422
step#423
step#424
step#425
step#426
step#427
step#428
step#429
step#429 -> loss: 271.631 / accuracy: 0.9565
step#430
step#431
step#432
step#433
step#434
step#435
step#436
step#437
step#438
step#439
step#439 -> loss: 270.995 / accuracy: 0.956
step#440
step#441
step#442
step#443
step#444
step#445
step#446
step#447
step#448
step#449
step#449 -> loss: 268.957 / accuracy: 0.956
step#450
step#451
step#452
step#453
step#454
step#455
step#456
step#457
step#458
step#459
step#459 -> loss: 263.568 / accuracy: 0.9575
step#460
step#461
step#462
step#463
step#464
step#465
step#466
step#467
step#468
step#469
step#469 -> loss: 263.336 / accuracy: 0.9585
step#470
step#471
step#472
step#473
step#474
step#475
step#476
step#477
step#478
step#479
step#479 -> loss: 264.117 / accuracy: 0.9545
step#480
step#481
step#482
step#483
step#484
step#485
step#486
step#487
step#488
step#489
step#489 -> loss: 256.762 / accuracy: 0.9635
step#490
step#491
step#492
step#493
step#494
step#495
step#496
step#497
step#498
step#499
step#499 -> loss: 255.662 / accuracy: 0.9595
294.63user 19.74system 5:30.86elapsed 95%CPU (0avgtext+0avgdata 1246440maxresident)k
0inputs+376outputs (0major+8378546minor)pagefaults 0swaps
294.74user 19.75system 5:30.98elapsed 95%CPU (0avgtext+0avgdata 1246440maxresident)k
0inputs+376outputs (0major+8382168minor)pagefaults 0swaps
2016年  2月 26日 金曜日 18:10:48 JST
